# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NAatkXYCrBJw8aM34OIh6waKbrqJ71m2

**Приведение датасета в порядок, работа с nan и анализ сбалансированности лейблов**
"""

import pandas as pd
data = pd.read_csv('dataset_utf-8 (1991).csv', sep=";")
data

data.label.value_counts()

data = data.drop(data[data['label'] == 'q'].index)
data = data.drop(data[data['label'] == '?'].index)
data.label.value_counts()

data['label'].isna().sum()
nan_rows = data[data['label'].isna()]
nan_rows
data = data.drop (index=[1712 , 1909])

data['label_list'] = data['label'].str.split(',')
label_freq = {}
for _, row in data.iterrows():
    labels = row['label_list']
    for label in labels:
        label = int(label.strip())
        if label in label_freq:
            label_freq[label] += 1
        else:
            label_freq[label] = 1
for label, freq in label_freq.items():
    print(f"Label {label}: {freq} occurrences")

data['list'] = data[data.columns[5:]].values.tolist()
new_data = data[['text', 'label']].copy()
new_data.head()

new_data = data[['text', 'label_list']].copy()
new_data

from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(new_data['label_list'])
print(list(mlb.classes_))
new_data['label_list1'] = list(y)
data_final = new_data[['text', 'label_list1']].copy()
data_final

"""**Распределение данных**"""

!pip install -q transformers
import numpy as np
from sklearn import metrics
import transformers
import torch
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertModel, BertConfig
from transformers import AutoTokenizer, AutoModel

from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'

MAX_LEN = 200
TRAIN_BATCH_SIZE = 8
VALID_BATCH_SIZE = 4
EPOCHS = 1
LEARNING_RATE = 1e-05
tokenizer = AutoTokenizer.from_pretrained("sberbank-ai/sbert_large_nlu_ru")
model = AutoModel.from_pretrained("sberbank-ai/sbert_large_nlu_ru")

class CustomDataset(Dataset):

    def __init__(self, dataframe, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.text = dataframe.text
        self.targets = self.data.label_list1
        self.max_len = max_len

    def __len__(self):
        return len(self.text)

    def __getitem__(self, index):
        text = str(self.text[index])
        text = " ".join(text.split())

        inputs = self.tokenizer.encode_plus(
            text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            pad_to_max_length=True,
            return_token_type_ids=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]


        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.float)
        }

train_size = 0.8
train_dataset=data_final.sample(frac=train_size,random_state=200)
test_dataset=data_final.drop(train_dataset.index).reset_index(drop=True)
train_dataset = train_dataset.reset_index(drop=True)

print("FULL Dataset: {}".format(data.shape))
print("TRAIN Dataset: {}".format(train_dataset.shape))
print("TEST Dataset: {}".format(test_dataset.shape))

training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)
testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)

train_params = {'batch_size': TRAIN_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }

test_params = {'batch_size': VALID_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }

training_loader = DataLoader(training_set, **train_params)
testing_loader = DataLoader(testing_set, **test_params)

"""**BERT**"""

class BERTClass(torch.nn.Module):
    def __init__(self):
        super(BERTClass, self).__init__()
        self.l1 = transformers.BertModel.from_pretrained('sberbank-ai/sbert_large_nlu_ru')
        self.l2 = torch.nn.Dropout(0.3)
        self.l3 = torch.nn.Linear(1024, 8)

    def forward(self, ids, mask, token_type_ids):
        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)
        output_2 = self.l2(output_1)
        output = self.l3(output_2)
        return output

model = BERTClass()
model.to(device)

def loss_fn(outputs, targets):
    return torch.nn.BCEWithLogitsLoss()(outputs, targets)

optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)

"""**Fine-tuning of the model**"""

def train(epoch):
    model.train()
    for _,data in enumerate(training_loader, 0):
        ids = data['ids'].to(device, dtype = torch.long)
        mask = data['mask'].to(device, dtype = torch.long)
        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
        targets = data['targets'].to(device, dtype = torch.float)

        outputs = model(ids, mask, token_type_ids)

        optimizer.zero_grad()
        loss = loss_fn(outputs, targets)
        if _%5000==0:
            print(f'Epoch: {epoch}, Loss:  {loss.item()}')

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

for epoch in range(EPOCHS):
    train(epoch)

"""**Validating the Model**"""

def validation(epoch):
    model.eval()
    fin_targets=[]
    fin_outputs=[]
    with torch.no_grad():
        for _, data in enumerate(testing_loader, 0):
            ids = data['ids'].to(device, dtype = torch.long)
            mask = data['mask'].to(device, dtype = torch.long)
            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)
            targets = data['targets'].to(device, dtype = torch.float)
            outputs = model(ids, mask, token_type_ids)
            fin_targets.extend(targets.cpu().detach().numpy().tolist())
            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
    return fin_outputs, fin_targets

for epoch in range(EPOCHS):
    outputs, targets = validation(epoch)
    outputs = np.array(outputs) >= 0.5
    accuracy = metrics.accuracy_score(targets, outputs)
    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')
    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')
    print(f"Accuracy Score = {accuracy}")
    print(f"F1 Score (Micro) = {f1_score_micro}")
    print(f"F1 Score (Macro) = {f1_score_macro}")

test_dataset
s = mlb.fit_transform(outputs)
test_dataset['label_list2'] = list(outputs)
test_dataset_with_outputs = test_dataset[['text', 'label_list1', 'label_list2']].copy()
test_dataset_with_outputs
#test_dataset_with_outputs.to_csv('test_dataset_with_outputs1.csv')

"""**Сохранение модели**"""

save_directory = "./save_pretrained"
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)

dir(model)

"""**Обучение на неразмеченном тексте**"""

import pandas as pd
data2 = pd.read_csv('dataset2.csv', sep=",")
data2

MAX_LEN2 = 5681

class CustomDataset2(Dataset):

    def __init__(self, dataframe, tokenizer, MAX_LEN2):
        self.tokenizer = tokenizer
        self.data2 = dataframe
        self.text = dataframe.text
        self.max_len2 = MAX_LEN2

    def __len__(self):
        return len(self.text)

    def __getitem__(self, index):
        text = str(self.text[index])
        text = " ".join(text.split())

        inputs = self.tokenizer.encode_plus(
            text,
            None,
            add_special_tokens=True,
            max_length2=self.max_len2,
            pad_to_max_length2=True,
            return_token_type_ids=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]


        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
        }

pr_data2 = CustomDataset2(data2, tokenizer, MAX_LEN2)

test_params2 = {'batch_size': VALID_BATCH_SIZE,
                'shuffle': True,
                'num_workers': 0
                }
testing_loader2 = DataLoader(pr_data2, **test_params2)

def validation(epoch):
    pt_model.eval()
    fin_outputs=[]
    with torch.no_grad():
        for _, data2 in enumerate(testing_loader2, 0):
            ids = data2['ids'].to(device, dtype = torch.long)
            mask = data2['mask'].to(device, dtype = torch.long)
            token_type_ids = data2['token_type_ids'].to(device, dtype = torch.long)
            outputs = pt_model(ids, mask, token_type_ids)
            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())
    return fin_outputs

s2 = mlb.fit_transform(outputs)
data2['outputs'] = list(outputs)
data2_with_outputs = data2[['text', 'outputs']].copy()
data2